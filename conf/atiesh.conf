# Example Configuration File of Atiesh Server #
#           Designed by whisperaven@gmail.com #

akka {
    # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
    # to STDOUT)
    loggers = ["akka.event.Logging$DefaultLogger"]
    # Log level used by the configured loggers (see "loggers") as soon
    # as they have been started; before that, see "stdout-loglevel"
    # Options: OFF, ERROR, WARNING, INFO, DEBUG
    loglevel = "OFF"

    actor {
        # Either one of "local", "remote" or "cluster" or the
        # FQCN of the ActorRefProvider to be used; the below is the built-in default,
        # note that "remote" and "cluster" requires the akka-remote and akka-cluster
        # artifacts to be on the classpath.
        provider = "local"

        default-dispatcher {
            # Must be one of the following
            # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
            # MessageDispatcherConfigurator with a public constructor with
            # both com.typesafe.config.Config parameter and
            # akka.dispatch.DispatcherPrerequisites parameters.
            # PinnedDispatcher must be used together with executor=thread-pool-executor.
            type = "Dispatcher"
            # Which kind of ExecutorService to use for this dispatcher
            # Valid options:
            #  - "default-executor" requires a "default-executor" section
            #  - "fork-join-executor" requires a "fork-join-executor" section
            #  - "thread-pool-executor" requires a "thread-pool-executor" section
            #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
            #  - A FQCN of a class extending ExecutorServiceConfigurator
            executor = "default-executor"
            # This will be used if you have set "executor = "default-executor"".
            # If an ActorSystem is created with a given ExecutionContext, this
            # ExecutionContext will be used as the default executor for all
            # dispatchers in the ActorSystem configured with
            # executor = "default-executor". Note that "default-executor"
            # is the default value for executor, and therefore used if not
            # specified otherwise. If no ExecutionContext is given,
            # the executor configured in "fallback" will be used.
            default-executor {
                fallback = "fork-join-executor"
            }

            # This will be used if you have set "executor = "fork-join-executor""
            # Underlying thread pool implementation is akka.dispatch.forkjoin.ForkJoinPool
            fork-join-executor {
                # Min number of threads to cap factor-based parallelism number to
                parallelism-min = 8

                # The parallelism factor is used to determine thread pool size using the
                # following formula: ceil(available processors * factor). Resulting size
                # is then bounded by the parallelism-min and parallelism-max values.
                parallelism-factor = 1.5

                # Max number of threads to cap factor-based parallelism number to
                parallelism-max = 128

                # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
                # like peeking mode which "pop".
                task-peeking-mode = "FIFO"
            }
        }

        default-blocking-io-dispatcher {
            type = Dispatcher
            executor = "thread-pool-executor"
            throughput = 100

            # This will be used if you have set "executor = "thread-pool-executor""
            # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
            thread-pool-executor {
                # Keep alive time for threads
                keep-alive-time = 60s

                # Define a fixed thread pool size with this property. The corePoolSize
                # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
                # value, if it is defined. Then the other pool-size properties will not
                # be used.
                #
                # Valid values are: `off` or a positive integer.
                fixed-pool-size = off

                # Min number of threads to cap factor-based corePoolSize number to
                core-pool-size-min = 8

                # The core-pool-size-factor is used to determine corePoolSize of the
                # ThreadPoolExecutor using the following formula:
                # ceil(available processors * factor).
                # Resulting size is then bounded by the core-pool-size-min and
                # core-pool-size-max values.
                core-pool-size-factor = 3.0

                # Max number of threads to cap factor-based corePoolSize number to
                core-pool-size-max = 128

                # Minimum number of threads to cap factor-based maximumPoolSize number to
                max-pool-size-min = 8

                # The max-pool-size-factor is used to determine maximumPoolSize of the
                # ThreadPoolExecutor using the following formula:
                # ceil(available processors * factor)
                # The maximumPoolSize will not be less than corePoolSize.
                # It is only used if using a bounded task queue.
                max-pool-size-factor  = 3.0

                # Max number of threads to cap factor-based maximumPoolSize number to
                max-pool-size-max = 128

                # Specifies the bounded capacity of the task queue (< 1 == unbounded)
                task-queue-size = -1

                # Specifies which type of task queue will be used, can be "array" or
                # "linked" (default)
                task-queue-type = "linked"

                # Allow core threads to time out
                allow-core-timeout = on
            }
        }

        default-pinned-dispatcher {
            type = "PinnedDispatcher"
            executor = "thread-pool-executor"
            thread-pool-executor.allow-core-timeout = off
        }
    }

    stream {
        # Default materializer settings
        materializer {
            # Initial size of buffers used in stream elements
            initial-input-buffer-size = 32
            # Maximum size of buffers used in stream elements
            max-input-buffer-size = 128

            # Fully qualified config path which holds the dispatcher configuration
            # to be used by ActorMaterializer when creating Actors.
            # When this value is left empty, the default-dispatcher will be used.
            dispatcher = ""

            # Enable additional troubleshooting logging at DEBUG log level
            debug-logging = off

            # Maximum number of elements emitted in batch if downstream signals large demand
            output-burst-limit = 1000

            # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
            # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
            # buffer upon stream materialization if the requested buffer size is less than this
            # configuration parameter. The default is very high because failing early is better
            # than failing under load.
            #
            # Buffers sized larger than this will dynamically grow/shrink and consume more memory
            # per element than the fixed size buffers.
            max-fixed-buffer-size = 1000000000

            # Maximum number of sync messages that actor can process for stream to substream communication.
            # Parameter allows to interrupt synchronous processing to get upsteam/downstream messages.
            # Allows to accelerate message processing that happening withing same actor but keep system responsive.
            sync-processing-limit = 1000
        }

        debug {
            # Enables the fuzzing mode which increases the chance of race conditions
            # by aggressively reordering events and making certain operations more
            # concurrent than usual.
            # This setting is for testing purposes, NEVER enable this in a production
            # environment!
            # To get the best results, try combining this setting with a throughput
            # of 1 on the corresponding dispatchers.
            fuzzing-mode = off
        }
        
        io.tcp {
            # The outgoing bytes are accumulated in a buffer while waiting for acknoledgment
            # of pending write. This improves throughput for small messages (frames) without
            # sacrificing latency. While waiting for the ack the stage will eagerly pull
            # from upstream until the buffer exceeds this size. That means that the buffer may hold
            # slightly more bytes than this limit (at most one element more). It can be set to 0
            # to disable the usage of the buffer.
            write-buffer-size = 128 KiB
        }

        default-blocking-io-dispatcher {
            type = "Dispatcher"
            executor = "thread-pool-executor"
            throughput = 1

            thread-pool-executor {
                fixed-pool-size = 32
            }
        }
    }

    # Akka installs JVM shutdown hooks by default, e.g. in CoordinatedShutdown and Artery. This property will
    # not disable user-provided hooks registered using `CoordinatedShutdown#addCancellableJvmShutdownHook`.
    # This property is related to `akka.coordinated-shutdown.run-by-jvm-shutdown-hook` below.
    # This property makes it possible to disable all such hooks if the application itself
    # or a higher level framework such as Play prefers to install the JVM shutdown hook and
    # terminate the ActorSystem itself, with or without using CoordinatedShutdown.
    jvm-shutdown-hooks = off

    # CoordinatedShutdown is an extension that will perform registered
    # tasks in the order that is defined by the phases. It is started
    # by calling CoordinatedShutdown(system).run(). This can be triggered
    # by different things, for example:
    # - JVM shutdown hook will by default run CoordinatedShutdown
    # - Cluster node will automatically run CoordinatedShutdown when it
    #   sees itself as Exiting
    # - A management console or other application specific command can
    #   run CoordinatedShutdown
    # coordinated-shutdown {
        # Run the coordinated shutdown when the JVM process exits, e.g.
        # via kill SIGTERM signal (SIGINT ctrl-c doesn't work).
        # This property is related to `akka.jvm-shutdown-hooks` above.
        # run-by-jvm-shutdown-hook = off
    # }

    http {
        client {
            # The default value of the `User-Agent` header to produce if no
            # explicit `User-Agent`-header was included in a request.
            # If this value is the empty string and no header was included in
            # the request, no `User-Agent` header will be rendered at all.
            user-agent-header = baishancloud/realtime-pusher

            # The time period within which the TCP connecting process must be completed.
            connecting-timeout = 5s

            # The time after which an idle connection will be automatically closed.
            # Set to `infinite` to completely disable idle timeouts.
            idle-timeout = 5 s

            # The initial size of the buffer to render the request headers in.
            # Can be used for fine-tuning request rendering performance but probably
            # doesn't have to be fiddled with in most applications.
            request-header-size-hint = 512

            # Socket options to set for the listening socket. If a setting is left
            # undefined, it will use whatever the default on the system is.
            socket-options {
                so-receive-buffer-size = undefined
                so-send-buffer-size = undefined
                so-reuse-address = undefined
                so-traffic-class = undefined
                tcp-keep-alive = undefined
                tcp-oob-inline = undefined
                tcp-no-delay = undefined
            }

            # Modify to tweak parsing settings on the client-side only.
            parsing {
              # no overrides by default, see `akka.http.parsing` for default values
            }

            # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
            # client for debugging reasons.
            #
            # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
            #
            # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
            # specify the number of bytes to log per chunk of data (the actual chunking depends
            # on implementation details and networking conditions and should be treated as
            # arbitrary).
            #
            # For logging on the server side, see akka.http.server.log-unencrypted-network-bytes.
            #
            # `off` : no log messages are produced
            # Int   : determines how many bytes should be logged per data chunk
            log-unencrypted-network-bytes = off
        }

        host-connection-pool {
            # The maximum number of parallel connections that a connection pool to a
            # single host endpoint is allowed to establish. Must be greater than zero.
            max-connections = 32

            # The minimum number of parallel connections that a pool should keep alive ("hot").
            # If the number of connections is falling below the given threshold, new ones are being spawned.
            # You can use this setting to build a hot pool of "always on" connections.
            # Default is 0, meaning there might be no active connection at given moment.
            # Keep in mind that `min-connections` should be smaller than `max-connections` or equal
            min-connections = 0

            # The maximum number of times failed requests are attempted again,
            # (if the request can be safely retried) before giving up and returning an error.
            # Set to zero to completely disable request retries.
            max-retries = 3

            # The maximum number of open requests accepted into the pool across all
            # materializations of any of its client flows.
            # Protects against (accidentally) overloading a single pool with too many client flow materializations.
            # Note that with N concurrent materializations the max number of open request in the pool
            # will never exceed N * max-connections * pipelining-limit.
            # Must be a power of 2 and > 0!
            max-open-requests = 32768

            # The maximum number of requests that are dispatched to the target host in
            # batch-mode across a single connection (HTTP pipelining).
            # A setting of 1 disables HTTP pipelining, since only one request per
            # connection can be "in flight" at any time.
            # Set to higher values to enable HTTP pipelining.
            pipelining-limit = 32

            # The time after which an idle connection pool (without pending requests)
            # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
            idle-timeout = infinite

            # The pool implementation to use. Currently supported are:
            #  - legacy: the original 10.0.x pool implementation
            #  - new: the pool implementation that became the default in 10.1.x and will receive fixes and new features
            pool-implementation = new

            # The "new" pool implementation will fail a connection early and clear the slot if a response entity was not
            # subscribed during the given time period after the response was dispatched. In busy systems the timeout might be
            # too tight if a response is not picked up quick enough after it was dispatched by the pool.
            response-entity-subscription-timeout = 1.second

            # Modify this section to tweak client settings only for host connection pools APIs like `Http().superPool` or
            # `Http().singleRequest`.
            client = {
                # no overrides by default, see `akka.http.client` for default values
                parsing {
                    # no overrides by default, see `akka.http.parsing` for default values
                }
            }
        }
    }
}

kamon {
    metric {
        # Interval at which metric snapshots will be collected and sent to all metric reporters.
        tick-interval = 5 seconds
    }

    modules {
        prometheus-reporter {
            enabled = true
            name = "Prometheus Reporter"
            description = "Exposes a scrape endpoint for Prometheus"
            factory = "kamon.prometheus.PrometheusReporter$Factory"
        }

        jvm-metrics {
            enabled = yes
        }

        process-metrics {
            enabled = no
        }

        host-metrics {
            enabled = no
        }
    }

    prometheus {
        # Enable or disable publishing the Prometheus scraping enpoint using a embedded server.
        start-embedded-http-server = yes
        # Enable of disable including tags from kamon.prometheus.environment as labels
        include-environment-tags = no

        embedded-server {
            # Hostname and port used by the embedded web server to publish the scraping enpoint.
            hostname = 0.0.0.0
            port = 9595
        }

        # Histogram buckets
        buckets {
            default-buckets = [
                60,
                120,
                300,
                600,
                900
            ]
        }
        # Per metric overrides are possible by specifying the metric name and the histogram buckets here
        custom {
            # example:
            # "akka.actor.processing-time" = [0.1, 1.0, 10.0]
        }
    }
}

atiesh {
    source {
        # generate "0" event 
        devzero {
            fqcn = "atiesh.source.DevZero"
            interceptors = ["transparent"]
            sinks = ["devnull"]
        }
    }

    interceptor {
        # Builtin Interceptors:
        #   transparent - log and passthrough all event 
        #   devnull     - log and discard all event
        transparent {
            fqcn = "atiesh.interceptor.Transparent"
            priority = 10
        }
        devnull {
            fqcn = "atiesh.interceptor.DevNull"
            priority = 10
        }
    }

    sink {
        # Builtin Sinks:
        #   devnull - accept everything, log and discard everything
        devnull {
            fqcn = "atiesh.sink.DevNull"
        }
    }

    # uncomment these lines for enable extensions
    # cached proxy for external system access & content cache
    extension {
        httpcache {
            # fqcn of the component
            fqcn = "atiesh.utils.http.HttpCachedProxy"
            # cache capacity (total slots)
            cache-size = 128
            # max response body size limit
            max-response-body-size = 32m
        }
    }
}
